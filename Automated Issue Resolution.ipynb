{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery basics\n",
    "\n",
    "[BigQuery](https://cloud.google.com/bigquery/docs/) is a petabyte-scale analytics data warehouse that you can use to run SQL queries over vast amounts of data in near realtime. This page shows you how to get started with the Google BigQuery API using the Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a client\n",
    "\n",
    "To use the BigQuery Python client library, start by initializing a client. The BigQuery client is used to send and receive messages from the BigQuery API.\n",
    "\n",
    "### Client project\n",
    "The `bigquery.Client` object uses your default project. Alternatively, you can specify a project in the `Client` constructor. For more information about how the default project is determined, see the [google-auth documentation](https://google-auth.readthedocs.io/en/latest/reference/google.auth.html).\n",
    "\n",
    "\n",
    "### Client location\n",
    "Locations are required for certain BigQuery operations such as creating a dataset. If a location is provided to the client when it is initialized, it will be the default location for jobs, datasets, and tables.\n",
    "\n",
    "Run the following to create a client with your default project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client = bigquery.Client(location=\"US\")\n",
    "#print(\"Client creating using default project: {}\".format(client.project))\n",
    "client = bigquery.Client(location=\"US\", project=\"prj-p-au-c360-anlytx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " !pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 ms, sys: 12.8 ms, total: 30.3 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME']='/opt/mapr/spark/spark'\n",
    "os.environ['PYSPARK_PYTHON']='/opt/python/python37/bin/python'\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import getpass\n",
    "import logging\n",
    "import numpy as np\n",
    "import calendar \n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import json\n",
    "from functools import partial\n",
    "import requests\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_choice(options):\n",
    "    \"\"\"\n",
    "    Prompts the user to choose from a list of predefined options or enter a custom value.\n",
    "    Args:\n",
    "        options (list): A list of predefined string options.\n",
    "    Returns:\n",
    "        str: The user's chosen string.\n",
    "    \"\"\"\n",
    "    num_options = len(options)\n",
    "    print(\"\\nPlease choose from the following options:\")\n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{i + 1}. {option}\")\n",
    "    print(f\"{num_options + 1}. Enter a custom value\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice_str = input(f\"Enter your choice (1-{num_options + 1}): \").strip()\n",
    "            if not choice_str:\n",
    "                print(\"Input cannot be empty. Please try again.\")\n",
    "                continue\n",
    "            choice = int(choice_str)\n",
    "            if 1 <= choice <= num_options:\n",
    "                return options[choice - 1]  # Return the chosen predefined option\n",
    "            elif choice == num_options + 1:\n",
    "                custom_value = input(\"Please enter your custom value: \").strip()\n",
    "                if not custom_value:\n",
    "                    print(\"Custom value cannot be empty. Please try again.\")\n",
    "                    continue\n",
    "                return custom_value  # Return the manually entered value\n",
    "            else:\n",
    "                print(f\"Invalid choice. Please enter a number between 1 and {num_options + 1}.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Please Provide User Input for Data Pull-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please choose from the following options:\n",
      "1. cm15\n",
      "2. iclic_id\n",
      "3. oblgr_id\n",
      "4. Enter a custom value\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-4):  2\n",
      "Enter the number of inputs (must be less than 11):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need to provide 2 value(s) for the 'iclic_id' field.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value 1 for 'iclic_id':  000236606885406\n",
      "Enter value 2 for 'iclic_id':  000268732174137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary ---\n",
      "You entered 2 input(s) for the field 'iclic_id'.\n",
      "Collected values: ['000236606885406', '000268732174137']\n"
     ]
    }
   ],
   "source": [
    "# Define your three options\n",
    "predefined_options = [\"cm15\", \"iclic_id\", \"oblgr_id\"]\n",
    "# Get the user's choice\n",
    "selected_value = get_user_choice(predefined_options)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        number_str = input(\"Enter the number of inputs (must be less than 11): \").strip()\n",
    "        if not number_str:\n",
    "            print(\"Input cannot be empty. Please try again.\")\n",
    "            continue # Go back to the start of the loop\n",
    "        number = int(number_str) # Attempt to convert to integer\n",
    "        if number < 11 and number > 0: # Ensure it's positive and less than 11\n",
    "            break # Exit the loop if valid\n",
    "        elif number <= 0:\n",
    "            print(\"Please enter a positive number of inputs.\")\n",
    "        else: # number is 11 or greater\n",
    "            print(\"Please enter a number less than 11.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a whole number.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "print(f\"You need to provide {number} value(s) for the '{selected_value}' field.\")\n",
    "# --- COLLECTING THE INPUT VALUES ---\n",
    "collected_data = []\n",
    "for i in range(number):\n",
    "    value = input(f\"Enter value {i+1} for '{selected_value}': \").strip()\n",
    "    if not value:\n",
    "        print(\"Input cannot be empty. Please enter a value.\")\n",
    "        # You might want to re-prompt for this specific value, or just append an empty string\n",
    "        # For simplicity, this example allows empty strings but you can add more validation\n",
    "    collected_data.append(value) # Appending the string value\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"You entered {number} input(s) for the field '{selected_value}'.\")\n",
    "print(f\"Collected values: {collected_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Data Pull query if Required-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'000236606885406','000268732174137'\n",
      "Number of rows: 2\n"
     ]
    }
   ],
   "source": [
    "formatted_list = \",\".join(f\"'{item}'\" if isinstance(item, str) else str(item) for item in collected_data)\n",
    "print(formatted_list)\n",
    "\n",
    "query = (f\"\"\"select cstone_last_updatetm,\n",
    "map_id,\n",
    "`axp-lumi`.dw.decrypt_sde(`axp-lumi`.dw.get_sde_tag('cm11', 'cust360_commercial'), cm11) as cm11,\n",
    "bu_in,\n",
    "`axp-lumi`.dw.decrypt_sde(`axp-lumi`.dw.get_sde_tag('cm15', 'cust360_commercial'), cm15) as cm15,\n",
    "cg_id,\n",
    "se10,\n",
    "iclic_id,oblgr_id,oblgr_nm,\n",
    "mtch_cd,\n",
    "mtch_cd_mi,\n",
    "cert_dun_no,\n",
    "`axp-lumi`.dw.decrypt_sde(`axp-lumi`.dw.get_sde_tag('tax_id', 'cust360_commercial'), tax_id) as tax_id,\n",
    "mkt_cd,\n",
    "bus_nm,\n",
    "json_value(bus_nm,\"$[0].source\") as nm_source1,\n",
    "json_value(bus_nm,\"$[0].source\") as nm_source1,\n",
    "json_value(bus_nm,\"$[0].type\") as nm_source_type1,\n",
    "json_value(bus_nm,\"$[0].bus_name\") as bus_nm1,\n",
    "json_value(bus_nm,\"$[1].source\") as nm_source2,\n",
    "json_value(bus_nm,\"$[1].type\") as nm_source_type2,\n",
    "json_value(bus_nm,\"$[1].bus_name\") as bus_nm2,\n",
    "json_value(bus_nm,\"$[2].source\") as nm_source3,\n",
    "json_value(bus_nm,\"$[2].type\") as nm_source_type3,\n",
    "json_value(bus_nm,\"$[2].bus_name\") as bus_nm3,\n",
    "json_value(bus_nm,\"$[3].source\") as nm_source4,\n",
    "json_value(bus_nm,\"$[3].type\") as nm_source_type4,\n",
    "json_value(bus_nm,\"$[3].bus_name\") as bus_nm4,\n",
    "json_value(bus_nm,\"$[4].source\") as nm_source5,\n",
    "json_value(bus_nm,\"$[4].type\") as nm_source_type5,\n",
    "json_value(bus_nm,\"$[4].bus_name\") as bus_nm5,\n",
    "`axp-lumi`.dw.decrypt_sde(`axp-lumi`.dw.get_sde_tag('bus_ad', 'cust360_commercial'), bus_ad) as bus_ad,\n",
    "`axp-lumi`.dw.decrypt_sde(`axp-lumi`.dw.get_sde_tag('bus_ad', 'cust360_commercial'), bus_full_ad) as bus_full_ad,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[0].source'),'\"','') as ad_source1,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[0].type'),'\"','') as ad_type1,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[0].full_address'),'\"','') as addr_line1,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[0].last_update_dt'),'\"','') as ad_last_update_dt1,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[1].source'),'\"','') as ad_source2,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[1].type'),'\"','') as ad_type2,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[1].full_address'),'\"','') as addr_line2,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[1].last_update_dt'),'\"','') as ad_last_update_dt2,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[2].source'),'\"','') as ad_source3,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[2].type'),'\"','') as ad_type3,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[2].full_address'),'\"','') as addr_line3,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[2].last_update_dt'),'\"','') as ad_last_update_dt3,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[3].source'),'\"','') as ad_source4,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[3].type'),'\"','') as ad_type4,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[3].full_address'),'\"','') as addr_line4,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[3].last_update_dt'),'\"','') as ad_last_update_dt4,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[4].source'),'\"','') as ad_source5,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[4].type'),'\"','') as ad_type5,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[4].full_address'),'\"','') as addr_line5,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[4].last_update_dt'),'\"','') as ad_last_update_dt5,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[5].source'),'\"','') as ad_source6,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[5].type'),'\"','') as ad_type6,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[5].full_address'),'\"','') as ad_full_address6,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[5].last_update_dt'),'\"','') as ad_last_update_dt6,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[6].source'),'\"','') as ad_source7,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[6].type'),'\"','') as ad_type7,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[6].full_address'),'\"','') as ad_full_address7,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[6].last_update_dt'),'\"','') as ad_last_update_dt7,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[7].source'),'\"','') as ad_source8,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[7].type'),'\"','') as ad_type8,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[7].full_address'),'\"','') as ad_full_address8,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[7].last_update_dt'),'\"','') as ad_last_update_dt8,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[8].source'),'\"','') as ad_source9,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[8].type'),'\"','') as ad_type9,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[8].full_address'),'\"','') as ad_full_address9,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[8].last_update_dt'),'\"','') as ad_last_update_dt9,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[9].source'),'\"','') as ad_source10,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[9].type'),'\"','') as ad_typ10,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[9].full_address'),'\"','') as ad_full_address10,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[9].last_update_dt'),'\"','') as ad_last_update_dt10,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[10].source'),'\"','') as ad_source11,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[10].type'),'\"','') as ad_typ11,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[10].full_address'),'\"','') as ad_full_address11,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[10].last_update_dt'),'\"','') as ad_last_update_dt11,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[11].source'),'\"','') as ad_source12,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[11].type'),'\"','') as ad_typ12,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[11].full_address'),'\"','') as ad_full_address12,\n",
    "replace(json_extract(`axp-lumi`.dw.decrypt_sde('NGBD-SDE-Address',bus_full_ad),'$[11].last_update_dt'),'\"','') as ad_last_update_dt12,\n",
    "prev_iclic_id,\n",
    "json_value(prev_iclic_id,\"$[1].iclic_id\") as prev_iclic,\n",
    "json_value(prev_iclic_id,\"$[0].update_dt\") as last_upd_dt,\n",
    "clnt_org_id,\n",
    "corp_id,\n",
    "iclic_mtch_cd,\n",
    "iclic_glbl_head_id,\n",
    "iclic_ult_id,\n",
    "lgl_ent_iclic_id,\n",
    "parnt_dun_no,\n",
    "hq_dun_no,\n",
    "dom_ult_dun_no,\n",
    "ult_dun_no,\n",
    "hq_parnt_nm,\n",
    "dom_ult_nm,\n",
    "glbl_ult_nm,\n",
    "acct_sta_cd,\n",
    "bus_acct_eff_dt,\n",
    "bus_acct_canc_dt,\n",
    "auth_offcr1_nat_id,\n",
    "auth_offcr1_nat_id_type,\n",
    "cg_id,\n",
    "corp_hier_cd,\n",
    "cust_xref_id,\n",
    "dun_no,\n",
    "prev_oblgr_id,\n",
    "rgn_loc,\n",
    "canc_rsn_cd,\n",
    "mkt_cd_no,\n",
    "dnb_mtch_grade,\n",
    "dnb_mtch_prfl,\n",
    "duns_cert_dt,\n",
    "sor_mkt_cd\n",
    "from `axp-lumi.dw.cust360_commercial`\n",
    "where {selected_value} in ({formatted_list})\n",
    "\"\"\")\n",
    "\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "df1 = query_job.to_dataframe()\n",
    "row_count = len(df1)\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for Commercial Linkage Assesment -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# --- Step 1. Ensure lists of dicts ---\n",
    "df1[\"bus_nm\"] = df1[\"bus_nm\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "df1[\"bus_ad\"] = df1[\"bus_ad\"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "# --- Step 2. Compact helper for addresses ---\n",
    "def build_address(addr):\n",
    "    return \", \".join([addr.get(k, \"\") for k in [\n",
    "        \"addr_line1\",\"addr_line2\",\"addr_line3\",\"addr_line4\",\n",
    "        \"addr_line5\",\"addr_line6\",\"city\",\"state\",\"country\",\"postal_code\"\n",
    "    ] if addr.get(k)])\n",
    "# --- Step 3. Extract in one pass (names + addresses) ---\n",
    "def extract_all(row):\n",
    "    legal_name, dba_name, home_addr,emboss_employer, business_addr = None, None, None, None, None\n",
    "    # Handle bus_nm\n",
    "    for nm in row[\"bus_nm\"] or []:\n",
    "        if nm.get(\"type\") in [\"legal_name\", \"LEGAL\"] and not legal_name:\n",
    "            legal_name = nm.get(\"bus_name\")\n",
    "        if nm.get(\"type\") in [\"DBA\", \"dba_nm\"] and not dba_name:\n",
    "            dba_name = nm.get(\"bus_name\")\n",
    "        if nm.get(\"type\") in [\"EMBOSS\", \"emboss name\", \"EMPLOYER\", \"employer name\"] and not emboss_employer:\n",
    "            emboss_employer = nm.get(\"bus_name\")\n",
    "            \n",
    "    # Handle bus_ad\n",
    "    for ad in row[\"bus_ad\"] or []:\n",
    "        if ad.get(\"type\") == \"HOME\" and not home_addr:\n",
    "            home_addr = build_address(ad)\n",
    "        if ad.get(\"type\") in [\"BUSINESS\", \"Business\"] and not business_addr:\n",
    "            business_addr = build_address(ad)\n",
    "    return pd.Series([legal_name, dba_name, home_addr,emboss_employer, business_addr])\n",
    "# --- Step 4. Apply once across rows ---\n",
    "df1[[\"legal_name\", \"dba_name\", \"Home_address\", \"emboss_employer\",\"Business_address\"]] = df1.apply(extract_all, axis=1)\n",
    "# --- Step 5. Final result ---\n",
    "df_final_result = df1\n",
    "row_count = len(df_final_result)\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "#df_final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF For Saving the Data in Excel Format -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_dataframe_to_excel(pandas_df, default_filename=\"my_report\"):\n",
    "    \"\"\"\n",
    "    Saves a Spark DataFrame to an Excel .xlsx file with user-defined name\n",
    "    and today's date appended.\n",
    "    Args:\n",
    "        spark_df (pyspark.sql.DataFrame): The Spark DataFrame to save.\n",
    "        default_filename (str): A default base filename if the user doesn't provide one.\n",
    "    \"\"\"\n",
    "    # --- 1. Get user input for base filename ---\n",
    "    from datetime import datetime\n",
    "    while True:\n",
    "        user_input = input(f\"Enter a base INC number for the Excel file (e.g., '{default_filename}'): \").strip()\n",
    "        if user_input:\n",
    "            base_filename = user_input\n",
    "            break\n",
    "        else:\n",
    "            print(\"Filename cannot be empty. Please try again.\")\n",
    "    # --- 2. Add today's date to the filename ---\n",
    "    today_date_str = datetime.now().strftime(\"%Y-%m-%d\") # Format: YYYY-MM-DD\n",
    "    excel_filename = f\"{base_filename}_{today_date_str}.xlsx\" # Using .xlsx for modern Excel format\n",
    "    # --- 3. Collect Spark DataFrame to Pandas DataFrame ---\n",
    "    print(f\"\\nCollecting Spark DataFrame to Pandas DataFrame. This may take time for large datasets...\")\n",
    "    try:\n",
    "        pandas_df\n",
    "        pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "        print(f\"Successfully collected {len(pandas_df)} rows to Pandas DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting Spark DataFrame to Pandas: {e}\")\n",
    "        print(\"The DataFrame might be too large to fit in memory.\")\n",
    "        return\n",
    "    # --- 4. Save Pandas DataFrame to Excel ---\n",
    "    try:\n",
    "        # Create the full path to save the file (e.g., in the current working directory)\n",
    "        #custom_path = r\"Z:\\Nucleus\\Prashant Saraswat\"\n",
    "        #output_path = os.path.join(custom_path, excel_filename)\n",
    "        # Using pandas to_excel method\n",
    "        Current = os.getcwd()\n",
    "        output_path = os.path.join(Current, excel_filename)\n",
    "        pandas_df.to_excel(output_path, index=False) # index=False prevents writing the Pandas DataFrame index as a column\n",
    "        print(f\"\\nDataFrame successfully saved to Excel: '{output_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Pandas DataFrame to Excel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Provide the File Name -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a base INC number for the Excel file (e.g., 'user_data_report'):  Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting Spark DataFrame to Pandas DataFrame. This may take time for large datasets...\n",
      "Successfully collected 2 rows to Pandas DataFrame.\n",
      "\n",
      "DataFrame successfully saved to Excel: '/home/jupyter/axp-us-psara6/Test_2026-01-15.xlsx'\n"
     ]
    }
   ],
   "source": [
    "save_dataframe_to_excel(df_final_result, default_filename=\"user_data_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final_result[\"cm15\"] = pd.to_numeric(df_final_result[\"cm15\"], errors=\"coerce\").astype(\"Int64\")\n",
    "new_df = df_final_result[['cm15','iclic_id','cg_id','se10','bu_in','tax_id', 'cert_dun_no','legal_name','dba_name','emboss_employer','Business_address','Home_address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Match Linkage Assesment\n",
    "## Please Provide the Record_Identifier/Field_Name - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the master record:  376011856655009\n",
      "Enter the name of the ID column (e.g., 'transaction_id'):  cm15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with match columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cm15</th>\n",
       "      <th>iclic_id</th>\n",
       "      <th>cg_id</th>\n",
       "      <th>se10</th>\n",
       "      <th>bu_in</th>\n",
       "      <th>tax_id</th>\n",
       "      <th>cert_dun_no</th>\n",
       "      <th>legal_name</th>\n",
       "      <th>dba_name</th>\n",
       "      <th>emboss_employer</th>\n",
       "      <th>Business_address</th>\n",
       "      <th>Home_address</th>\n",
       "      <th>tax_id_match</th>\n",
       "      <th>cert_dun_no_match</th>\n",
       "      <th>legal_name_match</th>\n",
       "      <th>dba_name_match</th>\n",
       "      <th>emboss_employer_match</th>\n",
       "      <th>Business_address_match</th>\n",
       "      <th>Home_address_match</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376011856655009</td>\n",
       "      <td>000268732174137</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>O</td>\n",
       "      <td>23481084076</td>\n",
       "      <td>742373395</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>THE TRUSTEE FOR THE JM CONSULTANCY SERVI</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340091130811001</td>\n",
       "      <td>000236606885406</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>O</td>\n",
       "      <td>64660784369</td>\n",
       "      <td>750699242</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>GRAYSTONE PRIVATE SC PTY LTD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cm15         iclic_id cg_id  se10 bu_in       tax_id  \\\n",
       "0  376011856655009  000268732174137  None  <NA>     O  23481084076   \n",
       "1  340091130811001  000236606885406  None  <NA>     O  64660784369   \n",
       "\n",
       "  cert_dun_no legal_name dba_name                           emboss_employer  \\\n",
       "0   742373395       None     None  THE TRUSTEE FOR THE JM CONSULTANCY SERVI   \n",
       "1   750699242       None     None              GRAYSTONE PRIVATE SC PTY LTD   \n",
       "\n",
       "  Business_address Home_address  tax_id_match  cert_dun_no_match  \\\n",
       "0             None         None          True               True   \n",
       "1             None         None         False              False   \n",
       "\n",
       "   legal_name_match  dba_name_match  emboss_employer_match  \\\n",
       "0             False           False                   True   \n",
       "1             False           False                  False   \n",
       "\n",
       "   Business_address_match  Home_address_match  is_match  \n",
       "0                   False               False     False  \n",
       "1                   False               False     False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def match_columns_from_master_record(df, record_id, id_column, columns_to_match):\n",
    "    \"\"\"\n",
    "    Matches specified columns in a DataFrame against a \"master\" record.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        record_id: The value in the id_column that identifies the master record.\n",
    "        id_column (str): The name of the column used to identify records (e.g., 'record_id').\n",
    "        columns_to_match (list): A list of column names to compare against the master record.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with individual match columns and a final boolean\n",
    "                      column 'is_match' indicating if the record matches the master record's\n",
    "                      values for all specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    # 1. Input validation to ensure the master record exists\n",
    "    if record_id not in df[id_column].values:\n",
    "        print(f\"Error: Record with ID '{record_id}' not found in column '{id_column}'.\")\n",
    "        return df\n",
    "\n",
    "    # 2. Identify the master record and extract its values for the specified columns\n",
    "    master_record = df[df[id_column] == record_id]\n",
    "\n",
    "    if master_record.empty:\n",
    "        print(f\"Error: Master record with ID '{record_id}' is empty.\")\n",
    "        return df\n",
    "\n",
    "    # 3. Create a master boolean mask to compare all records\n",
    "    # We initialize a mask of all True values\n",
    "    final_match_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    # 4. Iterate through the columns to match and create individual match columns\n",
    "    for col in columns_to_match:\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get the master record's value for the current column\n",
    "        master_value = master_record[col].iloc[0]\n",
    "\n",
    "        # Create a new boolean column for this specific match\n",
    "        individual_match_column_name = f\"{col}_match\"\n",
    "        df.loc[:, individual_match_column_name] = (df[col] == master_value)\n",
    "\n",
    "        # Update the final match mask by combining it with the individual match column\n",
    "        final_match_mask = final_match_mask & df[individual_match_column_name]\n",
    "\n",
    "    # 5. Add the 'is_match' column to the DataFrame\n",
    "    df.loc[:, \"is_match\"] = final_match_mask\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Get user input for master record details ---\n",
    "# The input is read as a string, so we need to convert it to an integer for matching.\n",
    "try:\n",
    "    master_id = int(input(\"Enter the master record: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a number for the transaction ID.\")\n",
    "    master_id = 103 # Default to 103 if input is invalid\n",
    "\n",
    "id_column = input(\"Enter the name of the ID column (e.g., 'transaction_id'): \")\n",
    "# --- END NEW ---\n",
    "\n",
    "# Assume the columns we want to match are 'product_category' and 'customer_country'.\n",
    "columns_to_check = ['tax_id', 'cert_dun_no','legal_name','dba_name','emboss_employer','Business_address','Home_address']\n",
    "\n",
    "# Call the function\n",
    "matched_df = match_columns_from_master_record(\n",
    "    df=new_df,\n",
    "    record_id=master_id,\n",
    "    id_column=id_column,\n",
    "    columns_to_match=columns_to_check\n",
    ")\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "print(\"\\nDataFrame with match columns:\")\n",
    "display(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = df_final_result[['cm15','iclic_id','cg_id','se10','bu_in','tax_id', 'cert_dun_no','legal_name','dba_name','emboss_employer','Business_address','Home_address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linkage Assesment Using Jaro Winkler - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with match columns:\n",
      "Number of rows: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jellyfish\n",
    "\n",
    "def match_columns_from_master_record(df, record_id, id_column, columns_to_match): \n",
    "    \"\"\"\n",
    "    Matches specified columns in a DataFrame against a \"master\" record using Jaro-Winkler similarity.\n",
    "    Produces similarity columns (0â€“100%) and a final average similarity_score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Always work on a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Input validation\n",
    "    if record_id not in df[id_column].values:\n",
    "        print(f\"Error: Record with ID '{record_id}' not found in column '{id_column}'.\")\n",
    "        return df\n",
    "\n",
    "    # 2. Get master record\n",
    "    master_record = df[df[id_column] == record_id]\n",
    "    if master_record.empty:\n",
    "        print(f\"Error: Master record with ID '{record_id}' is empty.\")\n",
    "        return df\n",
    "\n",
    "    # 3. Compare columns using Jaro-Winkler\n",
    "    for col in columns_to_match:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        master_value = str(master_record[col].iloc[0]) if pd.notna(master_record[col].iloc[0]) else \"\"\n",
    "        similarity_col_name = f\"{col}_similarity\"\n",
    "\n",
    "        # Assign similarity scores (percentage)\n",
    "        df[similarity_col_name] = df[col].fillna(\"\").astype(str).apply(\n",
    "            lambda x: jellyfish.jaro_winkler_similarity(str(x), master_value) * 100\n",
    "        )\n",
    "        \n",
    "        df[similarity_col_name] = df[similarity_col_name].round().astype(\"Int64\")\n",
    "\n",
    "\n",
    "    # 4. Add final similarity score (average of all similarity columns)\n",
    "    sim_cols = [c for c in df.columns if c.endswith(\"_similarity\")]\n",
    "    if sim_cols:\n",
    "        df[\"similarity_score\"] = (\n",
    "        df[sim_cols]\n",
    "        .replace(0, pd.NA)   # ðŸ‘ˆ ignore zero similarity values\n",
    "        .mean(axis=1)\n",
    "        .round()\n",
    "        .astype(\"Int64\")\n",
    "         )\n",
    "\n",
    "    return df\n",
    "\n",
    "#try:\n",
    " #   master_id = int(input(\"Enter the master record: \"))\n",
    "#except ValueError:\n",
    " #   print(\"Invalid input. Please enter a number for the transaction ID.\")\n",
    "  #  master_id = 103 # Default to 103 if input is invalid\n",
    "\n",
    "#id_column = input(\"Enter the name of the ID column (e.g., 'transaction_id'): \")\n",
    "# --- END NEW ---\n",
    "\n",
    "# Assume the columns we want to match are 'product_category' and 'customer_country'.\n",
    "columns_to_check = ['tax_id', 'cert_dun_no','legal_name','dba_name','emboss_employer','Business_address','Home_address']\n",
    "\n",
    "# Call the function\n",
    "matched_df = match_columns_from_master_record(\n",
    "    df=new_df,\n",
    "    record_id=master_id,\n",
    "    id_column=id_column,\n",
    "    columns_to_match=columns_to_check\n",
    ")\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)\n",
    "print(\"\\nDataFrame with match columns:\")\n",
    "row_count = len(matched_df)\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "#display(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ea44a_row0_col0, #T_ea44a_row0_col1, #T_ea44a_row0_col2, #T_ea44a_row0_col3, #T_ea44a_row0_col4, #T_ea44a_row0_col5, #T_ea44a_row0_col6, #T_ea44a_row0_col7, #T_ea44a_row0_col8, #T_ea44a_row0_col9, #T_ea44a_row0_col10, #T_ea44a_row0_col11, #T_ea44a_row0_col12, #T_ea44a_row0_col13, #T_ea44a_row0_col14, #T_ea44a_row0_col15, #T_ea44a_row0_col16, #T_ea44a_row0_col17, #T_ea44a_row0_col18, #T_ea44a_row0_col19 {\n",
       "  background-color: #90EE90;\n",
       "  color: red;\n",
       "}\n",
       "#T_ea44a_row1_col0, #T_ea44a_row1_col1, #T_ea44a_row1_col2, #T_ea44a_row1_col3, #T_ea44a_row1_col4, #T_ea44a_row1_col5, #T_ea44a_row1_col6, #T_ea44a_row1_col7, #T_ea44a_row1_col8, #T_ea44a_row1_col9, #T_ea44a_row1_col10, #T_ea44a_row1_col11, #T_ea44a_row1_col12, #T_ea44a_row1_col13, #T_ea44a_row1_col14, #T_ea44a_row1_col15, #T_ea44a_row1_col16, #T_ea44a_row1_col17, #T_ea44a_row1_col18, #T_ea44a_row1_col19 {\n",
       "  background-color: #90EE90;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ea44a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ea44a_level0_col0\" class=\"col_heading level0 col0\" >cm15</th>\n",
       "      <th id=\"T_ea44a_level0_col1\" class=\"col_heading level0 col1\" >iclic_id</th>\n",
       "      <th id=\"T_ea44a_level0_col2\" class=\"col_heading level0 col2\" >cg_id</th>\n",
       "      <th id=\"T_ea44a_level0_col3\" class=\"col_heading level0 col3\" >se10</th>\n",
       "      <th id=\"T_ea44a_level0_col4\" class=\"col_heading level0 col4\" >bu_in</th>\n",
       "      <th id=\"T_ea44a_level0_col5\" class=\"col_heading level0 col5\" >tax_id</th>\n",
       "      <th id=\"T_ea44a_level0_col6\" class=\"col_heading level0 col6\" >cert_dun_no</th>\n",
       "      <th id=\"T_ea44a_level0_col7\" class=\"col_heading level0 col7\" >legal_name</th>\n",
       "      <th id=\"T_ea44a_level0_col8\" class=\"col_heading level0 col8\" >dba_name</th>\n",
       "      <th id=\"T_ea44a_level0_col9\" class=\"col_heading level0 col9\" >emboss_employer</th>\n",
       "      <th id=\"T_ea44a_level0_col10\" class=\"col_heading level0 col10\" >Business_address</th>\n",
       "      <th id=\"T_ea44a_level0_col11\" class=\"col_heading level0 col11\" >Home_address</th>\n",
       "      <th id=\"T_ea44a_level0_col12\" class=\"col_heading level0 col12\" >tax_id_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col13\" class=\"col_heading level0 col13\" >cert_dun_no_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col14\" class=\"col_heading level0 col14\" >legal_name_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col15\" class=\"col_heading level0 col15\" >dba_name_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col16\" class=\"col_heading level0 col16\" >emboss_employer_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col17\" class=\"col_heading level0 col17\" >Business_address_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col18\" class=\"col_heading level0 col18\" >Home_address_similarity</th>\n",
       "      <th id=\"T_ea44a_level0_col19\" class=\"col_heading level0 col19\" >similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ea44a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ea44a_row0_col0\" class=\"data row0 col0\" >376011856655009</td>\n",
       "      <td id=\"T_ea44a_row0_col1\" class=\"data row0 col1\" >000268732174137</td>\n",
       "      <td id=\"T_ea44a_row0_col2\" class=\"data row0 col2\" >None</td>\n",
       "      <td id=\"T_ea44a_row0_col3\" class=\"data row0 col3\" ><NA></td>\n",
       "      <td id=\"T_ea44a_row0_col4\" class=\"data row0 col4\" >O</td>\n",
       "      <td id=\"T_ea44a_row0_col5\" class=\"data row0 col5\" >23481084076</td>\n",
       "      <td id=\"T_ea44a_row0_col6\" class=\"data row0 col6\" >742373395</td>\n",
       "      <td id=\"T_ea44a_row0_col7\" class=\"data row0 col7\" >None</td>\n",
       "      <td id=\"T_ea44a_row0_col8\" class=\"data row0 col8\" >None</td>\n",
       "      <td id=\"T_ea44a_row0_col9\" class=\"data row0 col9\" >THE TRUSTEE FOR THE JM CONSULTANCY SERVI</td>\n",
       "      <td id=\"T_ea44a_row0_col10\" class=\"data row0 col10\" >None</td>\n",
       "      <td id=\"T_ea44a_row0_col11\" class=\"data row0 col11\" >None</td>\n",
       "      <td id=\"T_ea44a_row0_col12\" class=\"data row0 col12\" >100</td>\n",
       "      <td id=\"T_ea44a_row0_col13\" class=\"data row0 col13\" >100</td>\n",
       "      <td id=\"T_ea44a_row0_col14\" class=\"data row0 col14\" >0</td>\n",
       "      <td id=\"T_ea44a_row0_col15\" class=\"data row0 col15\" >0</td>\n",
       "      <td id=\"T_ea44a_row0_col16\" class=\"data row0 col16\" >100</td>\n",
       "      <td id=\"T_ea44a_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "      <td id=\"T_ea44a_row0_col18\" class=\"data row0 col18\" >0</td>\n",
       "      <td id=\"T_ea44a_row0_col19\" class=\"data row0 col19\" >100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ea44a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ea44a_row1_col0\" class=\"data row1 col0\" >340091130811001</td>\n",
       "      <td id=\"T_ea44a_row1_col1\" class=\"data row1 col1\" >000236606885406</td>\n",
       "      <td id=\"T_ea44a_row1_col2\" class=\"data row1 col2\" >None</td>\n",
       "      <td id=\"T_ea44a_row1_col3\" class=\"data row1 col3\" ><NA></td>\n",
       "      <td id=\"T_ea44a_row1_col4\" class=\"data row1 col4\" >O</td>\n",
       "      <td id=\"T_ea44a_row1_col5\" class=\"data row1 col5\" >64660784369</td>\n",
       "      <td id=\"T_ea44a_row1_col6\" class=\"data row1 col6\" >750699242</td>\n",
       "      <td id=\"T_ea44a_row1_col7\" class=\"data row1 col7\" >None</td>\n",
       "      <td id=\"T_ea44a_row1_col8\" class=\"data row1 col8\" >None</td>\n",
       "      <td id=\"T_ea44a_row1_col9\" class=\"data row1 col9\" >GRAYSTONE PRIVATE SC PTY LTD</td>\n",
       "      <td id=\"T_ea44a_row1_col10\" class=\"data row1 col10\" >None</td>\n",
       "      <td id=\"T_ea44a_row1_col11\" class=\"data row1 col11\" >None</td>\n",
       "      <td id=\"T_ea44a_row1_col12\" class=\"data row1 col12\" >59</td>\n",
       "      <td id=\"T_ea44a_row1_col13\" class=\"data row1 col13\" >48</td>\n",
       "      <td id=\"T_ea44a_row1_col14\" class=\"data row1 col14\" >0</td>\n",
       "      <td id=\"T_ea44a_row1_col15\" class=\"data row1 col15\" >0</td>\n",
       "      <td id=\"T_ea44a_row1_col16\" class=\"data row1 col16\" >59</td>\n",
       "      <td id=\"T_ea44a_row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "      <td id=\"T_ea44a_row1_col18\" class=\"data row1 col18\" >0</td>\n",
       "      <td id=\"T_ea44a_row1_col19\" class=\"data row1 col19\" >55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6f14436bc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_rows(row):\n",
    "    if pd.notna(row[id_column]) and str(row[id_column]) == str(master_id):\n",
    "        return ['color: red']*len(row)\n",
    "    else:\n",
    "        return ['']*len(row)\n",
    "\n",
    "def highlight_rows_o(row):\n",
    "    if pd.notna(row['bu_in']) and row['bu_in'] == 'O':\n",
    "        return ['background-color: #90EE90']*len(row)\n",
    "    else:\n",
    "        return ['']*len(row)\n",
    "\n",
    "matched_df = matched_df.sort_values(by='bu_in', ascending=False)\n",
    "\n",
    "matched_df.style.apply(highlight_rows_o, axis=1).apply(highlight_rows, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No similar records exist â€” Delinking will be required.\n"
     ]
    }
   ],
   "source": [
    "# Ensure required columns exist and are properly typed\n",
    "if {\"similarity_score\", \"bu_in\"}.issubset(matched_df.columns):\n",
    "    # Convert similarity_score to numeric (in case it's Int64 or string)\n",
    "    matched_df[\"similarity_score\"] = pd.to_numeric(matched_df[\"similarity_score\"], errors=\"coerce\")\n",
    "\n",
    "    # Apply both conditions:\n",
    "    # 1ï¸âƒ£ similarity_score >= 80\n",
    "    # 2ï¸âƒ£ bu_in is not 'E'\n",
    "    # 3ï¸âƒ£ similarity_score != 80\n",
    "    filtered_df = matched_df[(matched_df[\"similarity_score\"] >= 80) & (matched_df[\"bu_in\"] != \"E\") & (matched_df[\"similarity_score\"] != 100) ]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No similar records exist â€” Delinking will be required.\")\n",
    "    else:\n",
    "        print(\"Similar records exist â€” Linkage is correct.\")\n",
    "else:\n",
    "    print(\"Missing one of the required columns: 'similarity_score' or 'bu_in'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCA - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid linkage rows found â€” checking master record for incorrect linkage reason...\n",
      "Linkage incorrect due to DnB (mtch_cd = '3', mtch_cd_mi = 'A')\n"
     ]
    }
   ],
   "source": [
    "if {\"similarity_score\", \"bu_in\"}.issubset(matched_df.columns):\n",
    "    # Convert similarity_score to numeric (in case it's Int64 or string)\n",
    "    matched_df[\"similarity_score\"] = pd.to_numeric(matched_df[\"similarity_score\"], errors=\"coerce\")\n",
    "\n",
    "    # Apply both conditions:\n",
    "    # 1ï¸âƒ£ similarity_score >= 80\n",
    "    # 2ï¸âƒ£ bu_in is not 'E'\n",
    "    # 3ï¸âƒ£ similarity_score != 80\n",
    "    filtered_df = matched_df[(matched_df[\"similarity_score\"] >= 80) & (matched_df[\"bu_in\"] != \"E\") & (matched_df[\"similarity_score\"] != 100) ]\n",
    "    # Run only if linkage was incorrect (no valid rows)\n",
    "    if filtered_df.empty:\n",
    "        print(\"No valid linkage rows found â€” checking master record for incorrect linkage reason...\")\n",
    "    \n",
    "        # Define mapping of (mtch_cd, mtch_cd_mi) â†’ print message\n",
    "        pair_to_msg = {\n",
    "            (\"3\", \"9\"): \"Linkage incorrect due to Arbitration\",\n",
    "            (\"\",  \"9\"): \"Linkage incorrect due to Mannual Changes\",\n",
    "            (\"3\", \"G\"): \"Linkage incorrect due to Internal Matching\",\n",
    "            (\"3\", \"A\"): \"Linkage incorrect due to DnB\"\n",
    "        }\n",
    "    \n",
    "        # Select only the master record from df_final_result\n",
    "        master_row = df_final_result[df_final_result[id_column] == master_id].copy()\n",
    "    \n",
    "        if master_row.empty:\n",
    "            print(f\"No master record found for ID '{master_id}'.\")\n",
    "        else:\n",
    "            # Ensure columns exist\n",
    "            if {\"mtch_cd\", \"mtch_cd_mi\"}.issubset(master_row.columns):\n",
    "                # Normalize for safe string comparison\n",
    "                master_row[\"mtch_cd\"] = master_row[\"mtch_cd\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\n",
    "                master_row[\"mtch_cd_mi\"] = master_row[\"mtch_cd_mi\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\n",
    "    \n",
    "                # Extract the pair for master record\n",
    "                mtch_cd_val = master_row[\"mtch_cd\"].iloc[0]\n",
    "                mtch_cd_mi_val = master_row[\"mtch_cd_mi\"].iloc[0]\n",
    "    \n",
    "                # Check if this pair matches any invalid mapping\n",
    "                message = pair_to_msg.get((mtch_cd_val, mtch_cd_mi_val))\n",
    "                if message:\n",
    "                    print(f\"{message} (mtch_cd = '{mtch_cd_val}', mtch_cd_mi = '{mtch_cd_mi_val}')\")\n",
    "                else:\n",
    "                    print(\"Master record linkage does not match any invalid mtch_cd / mtch_cd_mi combination.\")\n",
    "            else:\n",
    "                print(\"Columns 'mtch_cd' or 'mtch_cd_mi' not found in df_final_result.\")\n",
    "    else:\n",
    "        print(\"Linkage is correct â€” skipping mtch_cd / mtch_cd_mi check.\")\n",
    "        \n",
    "    def get_linkage_resolution(df_final_result, id_column, master_id, filtered_df):\n",
    "        \"\"\"\n",
    "        Returns the resolution string for the master record based on mtch_cd and mtch_cd_mi values.\n",
    "        Runs only if linkage is incorrect (filtered_df is empty).\n",
    "        \"\"\"\n",
    "    \n",
    "        if filtered_df.empty:\n",
    "            # Mapping of (mtch_cd, mtch_cd_mi) â†’ Resolution\n",
    "            pair_to_resolution = {\n",
    "                (\"3\", \"9\"): \"May require Manual Changes as matched due to arbitration\",\n",
    "                (\"\",  \"9\"): \"No Changes could be done in this case\",\n",
    "                (\"3\", \"G\"): \"Should be sent for Internal Rematching\",\n",
    "                (\"3\", \"A\"): \"Should be sent for External Rematching/Check the records at DnB portal\"\n",
    "            }\n",
    "    \n",
    "            # Select only the master record and make a copy to avoid SettingWithCopyWarning\n",
    "            master_row = df_final_result[df_final_result[id_column] == master_id].copy()\n",
    "    \n",
    "            if master_row.empty:\n",
    "                return f\"No master record found for ID '{master_id}'.\"\n",
    "    \n",
    "            if {\"mtch_cd\", \"mtch_cd_mi\"}.issubset(master_row.columns):\n",
    "                # Normalize values for comparison\n",
    "                mtch_cd_val = str(master_row[\"mtch_cd\"].iloc[0]) if pd.notna(master_row[\"mtch_cd\"].iloc[0]) else \"\"\n",
    "                mtch_cd_mi_val = str(master_row[\"mtch_cd_mi\"].iloc[0]) if pd.notna(master_row[\"mtch_cd_mi\"].iloc[0]) else \"\"\n",
    "    \n",
    "                # Normalize 'nan' literals\n",
    "                if mtch_cd_val.lower() == \"nan\":\n",
    "                    mtch_cd_val = \"\"\n",
    "                if mtch_cd_mi_val.lower() == \"nan\":\n",
    "                    mtch_cd_mi_val = \"\"\n",
    "    \n",
    "                # Return the matching resolution (or fallback)\n",
    "                resolution = pair_to_resolution.get((mtch_cd_val, mtch_cd_mi_val))\n",
    "                if resolution:\n",
    "                    return resolution\n",
    "                else:\n",
    "                    return (\n",
    "                        f\"No predefined resolution found for mtch_cd = '{mtch_cd_val}', \"\n",
    "                        f\"mtch_cd_mi = '{mtch_cd_mi_val}'.\"\n",
    "                    )\n",
    "            else:\n",
    "                return \"Columns 'mtch_cd' or 'mtch_cd_mi' not found in df_final_result.\"\n",
    "        else:\n",
    "            return \"Linkage is correct â€” no resolution lookup required.\"\n",
    "else:\n",
    "    print(\"Missing one of the required columns: 'similarity_score' or 'bu_in'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolution - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be sent for External Rematching/Check the records at DnB portal\n"
     ]
    }
   ],
   "source": [
    "resolution_message = get_linkage_resolution(df_final_result, id_column, master_id, filtered_df)\n",
    "print(resolution_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C360 API - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://gcprepo.aexp.com/pypi/simple\n",
      "Requirement already satisfied: requests-ntlm in /opt/conda/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm) (44.0.2)\n",
      "Requirement already satisfied: pyspnego>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm) (0.12.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm) (2.32.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm) (1.17.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->requests-ntlm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->requests-ntlm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->requests-ntlm) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->requests-ntlm) (2025.1.31)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-ntlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data for coordinates: 40.7128, -74.006\n",
      ":x: Error fetching data: HTTPSConnectionPool(host='api.open-meteo.com', port=443): Max retries exceeded with url: /v1/forecast?latitude=40.7128&longitude=-74.006&daily=temperature_2m_max&timezone=America%2FNew_York&forecast_days=7 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6f143c8f10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "\n",
      ":x: Failed to retrieve weather forecast data.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "# --- 1. Define the API Endpoint and Parameters ---\n",
    "# API for daily weather forecast (free and public)\n",
    "API_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "# Parameters for New York City (Latitude/Longitude)\n",
    "# We request the 'daily' forecast and specifically the 'temperature_2m_max'\n",
    "parameters = {\n",
    "    \"latitude\": 40.7128,    # New York City Latitude\n",
    "    \"longitude\": -74.0060, # New York City Longitude\n",
    "    \"daily\": \"temperature_2m_max\",\n",
    "    \"timezone\": \"America/New_York\",\n",
    "    \"forecast_days\": 7      # Request 7 days of data\n",
    "}\n",
    "def fetch_weather_data(url, params):\n",
    "    \"\"\"\n",
    "    Makes the API request and returns the parsed JSON data.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to fetch data for coordinates: {params['latitude']}, {params['longitude']}\")\n",
    "    try:\n",
    "        # Send a GET request with parameters\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status() # Check for HTTP errors\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\":x: Error fetching data: {e}\")\n",
    "        return None\n",
    "# --- Main Execution ---\n",
    "weather_data = fetch_weather_data(API_URL, parameters)\n",
    "if weather_data:\n",
    "    # 2. Extract and display the desired output\n",
    "    print(\"\\n:white_check_mark: API Request Successful!\")\n",
    "    # Extract the daily time stamps and max temperatures\n",
    "    daily_times = weather_data.get('daily', {}).get('time', [])\n",
    "    max_temps = weather_data.get('daily', {}).get('temperature_2m_max', [])\n",
    "    unit = weather_data.get('daily_units', {}).get('temperature_2m_max', 'Â°C')\n",
    "    print(\"\\n--- 7-Day Max Temperature Forecast for NYC ---\")\n",
    "    # Pair the date and temperature together for clean output\n",
    "    for date, temp in zip(daily_times, max_temps):\n",
    "        print(f\":date: {date}: {temp}{unit}\")\n",
    "else:\n",
    "    print(\"\\n:x: Failed to retrieve weather forecast data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid linkage rows found â€” checking master record for incorrect linkage reason...\n",
      "Linkage incorrect due to DnB (mtch_cd = '3', mtch_cd_mi = 'A')\n"
     ]
    }
   ],
   "source": [
    "# Run only if linkage was incorrect (no valid rows)\n",
    "if filtered_df.empty:\n",
    "    print(\"No valid linkage rows found â€” checking master record for incorrect linkage reason...\")\n",
    "\n",
    "    # Define mapping of (mtch_cd, mtch_cd_mi) â†’ print message\n",
    "    pair_to_msg = {\n",
    "        (\"3\", \"9\"): \"Linkage incorrect due to Arbitration\",\n",
    "        (\"\",  \"9\"): \"Linkage incorrect due to Mannual Changes\",\n",
    "        (\"3\", \"G\"): \"Linkage incorrect due to Internal Matching\",\n",
    "        (\"3\", \"A\"): \"Linkage incorrect due to DnB\"\n",
    "    }\n",
    "\n",
    "    # Select only the master record from df_final_result\n",
    "    master_row = df_final_result[df_final_result[id_column] == master_id].copy()\n",
    "\n",
    "    if master_row.empty:\n",
    "        print(f\"No master record found for ID '{master_id}'.\")\n",
    "    else:\n",
    "        # Ensure columns exist\n",
    "        if {\"mtch_cd\", \"mtch_cd_mi\"}.issubset(master_row.columns):\n",
    "            # Normalize for safe string comparison\n",
    "            master_row[\"mtch_cd\"] = master_row[\"mtch_cd\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\n",
    "            master_row[\"mtch_cd_mi\"] = master_row[\"mtch_cd_mi\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\n",
    "\n",
    "            # Extract the pair for master record\n",
    "            mtch_cd_val = master_row[\"mtch_cd\"].iloc[0]\n",
    "            mtch_cd_mi_val = master_row[\"mtch_cd_mi\"].iloc[0]\n",
    "\n",
    "            # Check if this pair matches any invalid mapping\n",
    "            message = pair_to_msg.get((mtch_cd_val, mtch_cd_mi_val))\n",
    "            if message:\n",
    "                print(f\"{message} (mtch_cd = '{mtch_cd_val}', mtch_cd_mi = '{mtch_cd_mi_val}')\")\n",
    "            else:\n",
    "                print(\"Master record linkage does not match any invalid mtch_cd / mtch_cd_mi combination.\")\n",
    "        else:\n",
    "            print(\"Columns 'mtch_cd' or 'mtch_cd_mi' not found in df_final_result.\")\n",
    "else:\n",
    "    print(\"Linkage is correct â€” skipping mtch_cd / mtch_cd_mi check.\")\n",
    "    \n",
    "def get_linkage_resolution(df_final_result, id_column, master_id, filtered_df):\n",
    "    \"\"\"\n",
    "    Returns the resolution string for the master record based on mtch_cd and mtch_cd_mi values.\n",
    "    Runs only if linkage is incorrect (filtered_df is empty).\n",
    "    \"\"\"\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        # Mapping of (mtch_cd, mtch_cd_mi) â†’ Resolution\n",
    "        pair_to_resolution = {\n",
    "            (\"3\", \"9\"): \"May require Manual Changes as matched due to arbitration\",\n",
    "            (\"\",  \"9\"): \"No Changes could be done in this case\",\n",
    "            (\"3\", \"G\"): \"Should be sent for Internal Rematching\",\n",
    "            (\"3\", \"A\"): \"Should be sent for External Rematching/Check the records at DnB portal\"\n",
    "        }\n",
    "\n",
    "        # Select only the master record and make a copy to avoid SettingWithCopyWarning\n",
    "        master_row = df_final_result[df_final_result[id_column] == master_id].copy()\n",
    "\n",
    "        if master_row.empty:\n",
    "            return f\"No master record found for ID '{master_id}'.\"\n",
    "\n",
    "        if {\"mtch_cd\", \"mtch_cd_mi\"}.issubset(master_row.columns):\n",
    "            # Normalize values for comparison\n",
    "            mtch_cd_val = str(master_row[\"mtch_cd\"].iloc[0]) if pd.notna(master_row[\"mtch_cd\"].iloc[0]) else \"\"\n",
    "            mtch_cd_mi_val = str(master_row[\"mtch_cd_mi\"].iloc[0]) if pd.notna(master_row[\"mtch_cd_mi\"].iloc[0]) else \"\"\n",
    "\n",
    "            # Normalize 'nan' literals\n",
    "            if mtch_cd_val.lower() == \"nan\":\n",
    "                mtch_cd_val = \"\"\n",
    "            if mtch_cd_mi_val.lower() == \"nan\":\n",
    "                mtch_cd_mi_val = \"\"\n",
    "\n",
    "            # Return the matching resolution (or fallback)\n",
    "            resolution = pair_to_resolution.get((mtch_cd_val, mtch_cd_mi_val))\n",
    "            if resolution:\n",
    "                return resolution\n",
    "            else:\n",
    "                return (\n",
    "                    f\"No predefined resolution found for mtch_cd = '{mtch_cd_val}', \"\n",
    "                    f\"mtch_cd_mi = '{mtch_cd_mi_val}'.\"\n",
    "                )\n",
    "        else:\n",
    "            return \"Columns 'mtch_cd' or 'mtch_cd_mi' not found in df_final_result.\"\n",
    "    else:\n",
    "        return \"Linkage is correct â€” no resolution lookup required.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
